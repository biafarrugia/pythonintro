{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python and Morph.io (for scraping)\n",
    "\n",
    "This tutorial is intended to build on some basic coding concepts and introduce Morph.io. By the end you should:\n",
    "\n",
    "* Be able to use GitHub to edit Python files\n",
    "* Use Morph.io to run Python code hosted on GitHub\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "\n",
    "## Get started with Morph.io\n",
    "\n",
    "1. [create an account on GitHub](https://github.com/) if you haven't got one already, and [sign in to Morph.io using your GitHub account](https://morph.io/users/auth/github)\n",
    "2. [Click **New scraper**](https://morph.io/scrapers/new) on the menu at the top of Morph.io. You will be taken to a new page asking you to specify more details\n",
    "3. On the dropdown menu for *Language*, select **Python**. Give your scraper a name in the next box - something like 'startingtocode' (no spaces), and in the final box write 'none' - this isn't a scraper yet: we're just using Morph.io as a place to learn code.\n",
    "4. Click **Create scraper**\n",
    "\n",
    "It will take Morph.io a few moments to create the files for your scraper (the files are being created on GitHub). \n",
    "\n",
    "When it has finished, you will be taken to a new page for the scraper. Look on the right where it says *Scraper code*. There should be a link to `startingtocode / scraper.py ` - this will take you to the pages on GitHub where the code is now hosted: `scraper.py` is the file for the code itself; `startingtocode` is the link to the repository containing that file.\n",
    "\n",
    "Open the link to `scraper.py` in a separate tab or window, but also keep your Morph.io page for this scraper open in another tab or window - you will need to edit the code on GitHub, and run it to see the results on Morph.io.\n",
    "\n",
    "Now we're ready to start.\n",
    "\n",
    "## Introducing the template code\n",
    "\n",
    "When you create a new scraper on Morph.io, it creates it with some template code as shown below. \n",
    "\n",
    "Each line begins with a hash symbol: `#`. There are two ways that these are most commonly used:\n",
    "\n",
    "* Firstly, as a way of creating **comments** in Python code: any code starting with a `#` does not do anything, so the hash symbol allows you to add comments which are not treated as working code.\n",
    "* Secondly, as a way of *disabling* code - what's called *commenting out* code. Rather than delete an entire line of code, it is easier to add a `#` at the front to turn it 'off' to test what happens, so you can always turn it back 'on' again quickly by removing the `#`.\n",
    "\n",
    "In the template code below generated by Morph.io, the *entire* code is commented out. The idea is that you can **uncomment** the sections you want to use in your own code, saving you time writing scraping code from scratch. We'll come back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "# import scraperwiki\n",
    "# import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "# html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "# root = lxml.html.fromstring(html)\n",
    "# root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries in Morph.io\n",
    "\n",
    "Make sure you are on this file in GitHub, and click the edit button to make some changes.\n",
    "\n",
    "Uncomment the two lines that start with `import` so the code looks like below.\n",
    "\n",
    "These two lines bring in two **libraries** to Morph.io: \n",
    "\n",
    "* Scraperwiki is a library which has useful functions for scraping webpages and storing the results in a database\n",
    "* lxml.html is a library which is useful for *parsing* HTML webpages - i.e. drilling down to particular pieces of information you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "# html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "# root = lxml.html.fromstring(html)\n",
    "# root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, uncomment the line `html = scraperwiki.scrape(\"http://foo.com\")`. \n",
    "\n",
    "This line is looking at a URL - foo.com - so it's worth checking that site in another window to see what's there.\n",
    "\n",
    "It's in parentheses, which means it's being used as an ingredient in a function - `scrape()`. Specifically, `scraperwiki.scrape()`, which means it's part of the **scraperwiki library**. \n",
    "\n",
    "When using a library it's always useful to check the **documentation** for that library - [here's the documentation for Scraperwiki](https://classic.scraperwiki.com/docs/python/), or at least it's 'Classic' version which was used by Morph.io. There's a link to [where the documentation is now hosted, on GitHub](https://github.com/scraperwiki/code-scraper-in-browser-tool/wiki)\n",
    "\n",
    "The `scrape` function grabs the contents of the given URL and stores it in the new variable `html`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "# root = lxml.html.fromstring(html)\n",
    "# root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **commit** your changes (GitHub's version of saving), and switch back to the scraper in Morph.io. Run the scraper.\n",
    "\n",
    "The first time you do this you may have to wait while Morph.io installs the libraries - but this only has to happen once and it should run more quickly the second time.\n",
    "\n",
    "*Note: if you get a 'status code 255' error then it means Morph.io isn't working properly right now. You can only leave it and come back later. (This doesn't happen that often).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section of code *converts* the `html` variable into another new variable called `root`, and drills down further into that using something called `cssselect`, which uses **css selectors** to grab very specific pieces of information from the page. We'll talk about this in class but search around for more about those selectors and think how they could be used in scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "root = lxml.html.fromstring(html)\n",
    "root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we see what's happening? \n",
    "\n",
    "Add a `print` command - or three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "root = lxml.html.fromstring(html)\n",
    "print html\n",
    "print root\n",
    "print root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first `print` command prints the variable `html` and the second `print` command prints the variable `root` - but the third doesn't print any variable at all. It is just added to the front of the line `root.cssselect(\"div[align='left']\")` - that's because the results of that line of code are not saved anywhere. \n",
    "\n",
    "We could save it in a variable first and then print it. How would you do that?\n",
    "\n",
    "Anyway, the results of those last two print commands are a bit cryptic. Here's one:\n",
    "\n",
    "`<Element html at 0x7faa5365c470>`\n",
    "\n",
    "And here's the other:\n",
    "\n",
    "`[]`\n",
    "\n",
    "The `<Element html at 0x7faa5365c470>` bit looks funny because `root` has been created with an `lxml.html` function: it what we call an lxml.html *object*. So what this tells us is that we need to find some way to convert or decode this information back into something understandable.\n",
    "\n",
    "In theory that's what `cssselect` should do. But the results of that are pretty unimpressive: `[]`\n",
    "\n",
    "What can we work out from that? The square brackets are a big clue. Square brackets indicate a **list**, and that's exactly what `cssselect` generates: a list of elements that match a css selector.\n",
    "\n",
    "But there's nothing in those square brackets, so our list is **empty**. Why? Because `cssselect` found *no* matches. \n",
    "\n",
    "Look at the code: `cssselect` was looking for this: `\"div[align='left']\"`. In other words, any content inside the HTML tags `<div align=\"left\">`.\n",
    "\n",
    "Check the source code of the webpage that is being scraped: foo.com. Are there any such tags? No. \n",
    "\n",
    "We can alter it, then, to look for a tag we *know* is on that page: link tags for example. The css selector for a link is `a` (as in `<a href=`), so we can alter the code like so:\n",
    "\n",
    "`root.cssselect(\"a\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "root = lxml.html.fromstring(html)\n",
    "print html\n",
    "print root\n",
    "print root.cssselect(\"a\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we get a different result:\n",
    "\n",
    "`[<Element a at 0x7f80ac4b4838>, <Element a at 0x7f80ac4b4890>]`\n",
    "\n",
    "Once again those square brackets tell us this is a list, and we can count two items in that list now. Checking the source HTML on the webpage we can see there are two `a` tags there too. But again these have been encoded as lxml objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it's a list then we need to loop through it. We also need to store it in a variable first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element a at 0x10708c5e8>\n",
      "<Element a at 0x109fc0c78>\n"
     ]
    }
   ],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "root = lxml.html.fromstring(html)\n",
    "#I've commented out the 3 lines below so we can focus on the result of the new ones\n",
    "#print html\n",
    "#print root\n",
    "#print root.cssselect(\"a\")\n",
    "alistoflinks = root.cssselect(\"a\")\n",
    "for link in alistoflinks:\n",
    "    print link\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decode them *back* from lxml we need to use some lxml code: `.text` shows the text *within* the HTML tags that have been grabbed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foo.com\n",
      "Privacy Policy\n"
     ]
    }
   ],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "root = lxml.html.fromstring(html)\n",
    "#I've commented out the 3 lines below so we can focus on the result of the new ones\n",
    "#print html\n",
    "#print root\n",
    "#print root.cssselect(\"a\")\n",
    "alistoflinks = root.cssselect(\"a\")\n",
    "for link in alistoflinks:\n",
    "    print link.text\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `.text` will show you the contents of a tag, but *not* any text inside other tags *inside* the tag. For example, if the HTML was `<p>My friend <em>Paul</em></p>` and we were grabbing all the `<p>` tags, `.text` would only show 'My friend'.\n",
    "\n",
    "An alternative, then, is `.text_content()`, which shows the text content of the tag *including any context inside child tags*. Here's the code with that instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foo.com\n",
      "Privacy Policy\n"
     ]
    }
   ],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "root = lxml.html.fromstring(html)\n",
    "#I've commented out the 3 lines below so we can focus on the result of the new ones\n",
    "#print html\n",
    "#print root\n",
    "#print root.cssselect(\"a\")\n",
    "alistoflinks = root.cssselect(\"a\")\n",
    "for link in alistoflinks:\n",
    "    print link.text_content()\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third useful piece of code is `.attrib`, which allows us to grab an attribute of the HTML tag (which needs to be specified as a string inside square brackets like so: `.attrib['href']`). \n",
    "\n",
    "Here is the same code again adapted to use that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "root = lxml.html.fromstring(html)\n",
    "#I've commented out the 3 lines below so we can focus on the result of the new ones\n",
    "#print html\n",
    "#print root\n",
    "#print root.cssselect(\"a\")\n",
    "alistoflinks = root.cssselect(\"a\")\n",
    "for link in alistoflinks:\n",
    "    print link.attrib['href']\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just those 3 you can grab most of the information that you might want to scrape. For other options [see the documentation on parsing HTML with the lxml library](http://lxml.de/parsing.html)\n",
    "\n",
    "## Storing the results\n",
    "\n",
    "Now that we've grabbed the links we need some way to store them.\n",
    "\n",
    "We can do that for each piece of information in the usual way, like so: \n",
    "\n",
    "`linktext = link.text_content()`\n",
    "\n",
    "However, when it comes to storing multiple pieces of information we need a different type of variable - we need a **dictionary** variable. \n",
    "\n",
    "Remember that a dictionary variable stores a series of **key-value** pairs. The key is like a column heading; the value is like a cell that would be in that column. Here's an example:\n",
    "\n",
    "`{'link': '/digimedia_privacy_policy.html', 'linktext': 'Privacy Policy'}`\n",
    "\n",
    "This is the type of variable we need to store information in a database. \n",
    "\n",
    "First, then, we create a empty dictionary variable that we choose to call 'record':\n",
    "\n",
    "`record = {}`\n",
    "\n",
    "To add new data to this, we have to name the *key* inside square brackets after the name of the dictionary variable like so: `record['linktext']` - then, after an equals sign, the *value* that we want to store against that key. Taken together it looks like this:\n",
    "\n",
    "`record['linktext'] = link.text_content()`\n",
    "\n",
    "Now that we have data in the dictionary variable, we can store it in the scraperwiki database. The code for that takes some explaining:\n",
    "\n",
    "`scraperwiki.sqlite.save(unique_keys=['link'], data=record)`\n",
    "\n",
    "First, we have the function that is needed to save to the database: `scraperwiki.sqlite.save()`. \n",
    "\n",
    "The detail comes inside those brackets: we need to specify what data we are saving and - before that - the *unique* key from that dictionary variable.\n",
    "\n",
    "`unique_keys=['link']`, then, specifies that `'link'` is going to be the unique key. This means if it comes across the same link twice it will only save one of those. Ideally we want something that isn't going to recur, or something that we only want to store once. \n",
    "\n",
    "`data=record` specifies that our data is in a variable called `record`.\n",
    "\n",
    "Here's that code in action - I'm going to explain the looping bit in a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'link': '/', 'linktext': 'Foo.com'}\n",
      "{'link': '/digimedia_privacy_policy.html', 'linktext': 'Privacy Policy'}\n"
     ]
    }
   ],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "root = lxml.html.fromstring(html)\n",
    "#I've commented out the 3 lines below so we can focus on the result of the new ones\n",
    "#print html\n",
    "#print root\n",
    "#print root.cssselect(\"a\")\n",
    "alistoflinks = root.cssselect(\"a\")\n",
    "record = {}\n",
    "for link in alistoflinks:\n",
    "    record['linktext'] = link.text_content()\n",
    "    record['link'] = link.attrib['href']\n",
    "    print record\n",
    "    scraperwiki.sqlite.save(unique_keys=['link'], data=record)\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final thing to emphasise is that our code to `save` data is placed within a *loop*. This is because the scraper saves *one row of data at a time*. Let's recap:\n",
    "\n",
    "1. We have 3 lines that steadily narrow down to the specific data that we want to store: first we grab the whole webpage (`html = scraperwiki.scrape(\"http://foo.com\")`); then we convert it into an lxml object (`root = lxml.html.fromstring(html)`); and then we grab all the 'a' tags (`alistoflinks = root.cssselect(\"a\")`)\n",
    "2. We create an empty dictionary variable, ready to be filled in a moment: `record = {}`\n",
    "3. We begin looping through that list of a tags: `for link in alistoflinks:`\n",
    "4. We fill that empty dictionary with two items *from the first item in the list*: first the text of the 'a' tag (`record['linktext'] = link.text_content()`) and second the href attribute of the a tag (`record['link'] = link.attrib['href']`).\n",
    "5. We `print record` to see what it now looks like. It is like one row of a table: `{'link': '/', 'linktext': 'Foo.com'}`\n",
    "6. We save that 'record' variable: `scraperwiki.sqlite.save(unique_keys=['link'], data=record)`\n",
    "7. The loop runs again on the *second item in the list*. The variable 'record' is overwritten with info from that second item. When *this* is printed it looks like a different row in the same table: `{'link': '/digimedia_privacy_policy.html', 'linktext': 'Privacy Policy'}`\n",
    "8. If there were more items in the list it would continue looping, and we would get more and more data in the database. \n",
    "\n",
    "Notice a key difference here: the variable `record` only ever contains 2 keys and 2 corresponding values. Those values keep changing as the loop runs, but at the end it still only has 2 values.\n",
    "\n",
    "The database, however, contains the 2 keys plus *all* the values that the `record` variable *ever* contained. It can now be queried, or added to, or altered."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
