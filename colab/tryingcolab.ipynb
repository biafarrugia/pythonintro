{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tryingcolab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5WbovDszEt-",
        "colab_type": "text"
      },
      "source": [
        "# Testing an old scraper\n",
        "\n",
        "This code was used before on Quickcode. I'm pasting into Colab.\n",
        "\n",
        "First, import the `scraperwiki` library. We need to [install it first](https://colab.research.google.com/notebooks/snippets/importing_libraries.ipynb) too otherwise we get an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq0IoGovyiWz",
        "colab_type": "code",
        "outputId": "0b3fcf50-abc5-40fd-c366-d8ace5f59ec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "#install scraperwiki\n",
        "!pip install scraperwiki\n",
        "import scraperwiki"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scraperwiki\n",
            "  Downloading https://files.pythonhosted.org/packages/30/84/d874847baad89f03e6984fcd87505a37bf924b66519d1e07bf76e2369af0/scraperwiki-0.5.1.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from scraperwiki) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from scraperwiki) (1.12.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from scraperwiki) (1.3.11)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/64/493c45119dce700a4b9eeecc436ef9e8835ab67bae6414f040cdc7b58f4b/alembic-1.3.1.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->scraperwiki) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->scraperwiki) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->scraperwiki) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->scraperwiki) (1.24.3)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/3c/8dcd6883d009f7cae0f3157fb53e9afb05a0d3d33b3db1268ec2e6f4a56b/Mako-1.1.0.tar.gz (463kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 41.8MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->scraperwiki) (2.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->scraperwiki) (1.1.1)\n",
            "Building wheels for collected packages: scraperwiki, alembic, Mako\n",
            "  Building wheel for scraperwiki (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scraperwiki: filename=scraperwiki-0.5.1-cp36-none-any.whl size=6547 sha256=98a8c4384c1b1fb49511ed406a421bfc25294f05bf498023003f695a9308f0e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/6e/60/e13b585339206922e816bb90c355b79aa077ab2b15d7cc26a7\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.3.1-py2.py3-none-any.whl size=144523 sha256=37ea09905384ed15340e64c625b162cda15de42d943e58c3c496a7e34a6c1bb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/d4/19/5ab879d30af7cbc79e6dcc1d421795b1aa9d78f455b0412ef7\n",
            "  Building wheel for Mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Mako: filename=Mako-1.1.0-cp36-none-any.whl size=75363 sha256=2129be7dbb10a1be98672891cc514704a6ca4b6c544924232de6f893960d5f38\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/32/7b/a291926643fc1d1e02593e0d9e247c5a866a366b8343b7aa27\n",
            "Successfully built scraperwiki alembic Mako\n",
            "Installing collected packages: Mako, python-editor, alembic, scraperwiki\n",
            "Successfully installed Mako-1.1.0 alembic-1.3.1 python-editor-1.0.4 scraperwiki-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydBcDmqwzADK",
        "colab_type": "text"
      },
      "source": [
        "Now let's test something else."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb-R_nrBzBy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lxml.html\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bui8kdwQzXSV",
        "colab_type": "text"
      },
      "source": [
        "Those import OK.\n",
        "\n",
        "Now `cssselect` used to be in `lxml.html` but [now it isn't](https://stackoverflow.com/questions/23221073/how-to-fix-issue-with-the-removed-cssselect-package-in-lxml), so we need to import it separately. This again needs to be installed too, to prevent an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brVm23Cu0nM_",
        "colab_type": "code",
        "outputId": "9d42e8a3-b451-4451-98dd-89649b6cb371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "!pip install cssselect\n",
        "import cssselect"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cssselect\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: cssselect\n",
            "Successfully installed cssselect-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm9xfDpa0s2G",
        "colab_type": "text"
      },
      "source": [
        "Let's try to create some basic variables we need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGkKR-RKzZUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hits a problem with\n",
        "#http://trains.im/ppmhistorical/GW/34\n",
        "#Because the page is empty - this is dealt with in the code\n",
        "\n",
        "#This will be needed for any relative links\n",
        "baseurl = \"http://trains.im\"\n",
        "#Start here to gather the URLs for all the train company webpages\n",
        "starturl = \"http://trains.im/ppmhistorical/\"\n",
        "#This list was generated by scraping the above URL using IMPORTXML for all the links, and then extracting the codes using SPLIT and INDEX, then using JOIN to make into a Python list\n",
        "codelist = ['AW','CC','CH','XC','GR','EM','ES','ET','GW','SR','GC','LE','HC','HX','HT','LO','ME','NT','SW','SE','TP','VT','CS','XR','WM']\n",
        "\n",
        "#This code can be run to gather the codes, but is now commented out as the results are stored in a list below in routecodes_archive \n",
        "'''\n",
        "#Create an empty list to store the route codes\n",
        "routecodes = []\n",
        "for i in codelist:\n",
        "    codeurl = starturl+i\n",
        "    print(codeurl)\n",
        "    html = scraperwiki.scrape(codeurl)\n",
        "    root = lxml.html.fromstring(html) # turn our HTML into an lxml object\n",
        "    links = root.cssselect('ul.nav.nav-list li a') # get all the links inside li tags  \n",
        "    for link in links:\n",
        "        routelink = link.attrib['href']\n",
        "        print(routelink)\n",
        "        routecodes.append(routelink)\n",
        "\n",
        "print(routecodes)\n",
        "'''\n",
        "\n",
        "#I've copied the results below so we don't have to generate it every time\n",
        "routecodes_archive = ['/ppmhistorical/CC/8', '/ppmhistorical/CH/10', '/ppmhistorical/CH/11', '/ppmhistorical/CH/12', '/ppmhistorical/CH/13', '/ppmhistorical/XC/14', '/ppmhistorical/XC/15', '/ppmhistorical/GR/100', '/ppmhistorical/GR/101', '/ppmhistorical/GR/102', '/ppmhistorical/GR/103', '/ppmhistorical/EM/16', '/ppmhistorical/EM/17', '/ppmhistorical/EM/18', '/ppmhistorical/EM/19', '/ppmhistorical/EM/20', '/ppmhistorical/ET/21', '/ppmhistorical/ET/22', '/ppmhistorical/ET/23', '/ppmhistorical/ET/24', '/ppmhistorical/ET/25', '/ppmhistorical/ET/26', '/ppmhistorical/ET/27', '/ppmhistorical/ET/28', '/ppmhistorical/ET/29', '/ppmhistorical/ET/110', '/ppmhistorical/ET/111', '/ppmhistorical/ET/112', '/ppmhistorical/ET/113', '/ppmhistorical/GW/32', '/ppmhistorical/GW/33', '/ppmhistorical/GW/34', '/ppmhistorical/GW/35', '/ppmhistorical/GW/36', '/ppmhistorical/GW/37', '/ppmhistorical/GW/38', '/ppmhistorical/GW/39', '/ppmhistorical/GW/40', '/ppmhistorical/GW/41', '/ppmhistorical/GW/42', '/ppmhistorical/GW/43', '/ppmhistorical/GW/44', '/ppmhistorical/GW/45', '/ppmhistorical/SR/76', '/ppmhistorical/SR/77', '/ppmhistorical/SR/78', '/ppmhistorical/SR/79', '/ppmhistorical/SR/80', '/ppmhistorical/SR/81', '/ppmhistorical/SR/82', '/ppmhistorical/GC/30', '/ppmhistorical/GC/31', '/ppmhistorical/LE/46', '/ppmhistorical/LE/47', '/ppmhistorical/LE/48', '/ppmhistorical/LE/49', '/ppmhistorical/LE/50', '/ppmhistorical/LE/51', '/ppmhistorical/HC/52', '/ppmhistorical/LO/59', '/ppmhistorical/LO/60', '/ppmhistorical/LO/61', '/ppmhistorical/LO/62', '/ppmhistorical/LO/63', '/ppmhistorical/ME/64', '/ppmhistorical/ME/65', '/ppmhistorical/NT/66', '/ppmhistorical/NT/67', '/ppmhistorical/NT/68', '/ppmhistorical/NT/69', '/ppmhistorical/NT/70', '/ppmhistorical/NT/71', '/ppmhistorical/NT/72', '/ppmhistorical/NT/73', '/ppmhistorical/NT/74', '/ppmhistorical/NT/75', '/ppmhistorical/SW/83', '/ppmhistorical/SW/84', '/ppmhistorical/SW/85', '/ppmhistorical/SW/86', '/ppmhistorical/SW/87', '/ppmhistorical/SW/88', '/ppmhistorical/SW/89', '/ppmhistorical/SW/90', '/ppmhistorical/SE/91', '/ppmhistorical/SE/92', '/ppmhistorical/SE/93', '/ppmhistorical/SE/94', '/ppmhistorical/SE/95', '/ppmhistorical/TP/97', '/ppmhistorical/TP/98', '/ppmhistorical/TP/99', '/ppmhistorical/VT/104', '/ppmhistorical/VT/105', '/ppmhistorical/VT/106', '/ppmhistorical/VT/107', '/ppmhistorical/VT/108', '/ppmhistorical/VT/109', '/ppmhistorical/CS/9', '/ppmhistorical/XR/96', '/ppmhistorical/XR/114', '/ppmhistorical/WM/53', '/ppmhistorical/WM/54', '/ppmhistorical/WM/55', '/ppmhistorical/WM/56', '/ppmhistorical/WM/57', '/ppmhistorical/WM/58']\n",
        "#testurl - no longer needed\n",
        "testurl = \"http://trains.im/ppmhistorical/WM/57\"\n",
        "#This is empty and trips up the scraper, \n",
        "#which is why we have an if-else test in the for loop at the bottom of this scraper\n",
        "#used for testing that loop\n",
        "emptyurl = \"http://trains.im/ppmhistorical/GW/34\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "480pB6yMzo-1",
        "colab_type": "text"
      },
      "source": [
        "## Defining functions\n",
        "\n",
        "And now let's define some functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaWd_HYuzsKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a function to scrape the route page for the data that generates the chart\n",
        "def scrapechartpage(url):\n",
        "    print(\"scraping\",url)\n",
        "    html = scraperwiki.scrape(url)\n",
        "    root = lxml.html.fromstring(html) # turn our HTML into an lxml object\n",
        "    h3s = root.cssselect('h3') # get all the <h3> tags\n",
        "    h3 = h3s[1].text_content()\n",
        "    print(h3)\n",
        "    scripts = root.cssselect('script') # get all the <script> tags\n",
        "    for script in scripts:\n",
        "        scriptxt = script.text_content()\n",
        "        print('SCRIPT:',scriptxt)\n",
        "        if \"data: \" in scriptxt:\n",
        "            morrisarea = script.text_content().split(\"data: \")[1].split(\"xkey: 'Date',\")[0]\n",
        "            print(morrisarea)\n",
        "            return(morrisarea, h3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEcfZ6Kvz0ut",
        "colab_type": "text"
      },
      "source": [
        "## The first loop\n",
        "\n",
        "We create a `for` loop to go through the list we created and run the function on them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk_tELGMz0W1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in routecodes_archive:\n",
        "    fullurl = baseurl+i\n",
        "    chartjson, heading = scrapechartpage(fullurl)\n",
        "    print(len(chartjson.split(\"],\")[0].split(\"[\")))\n",
        "    if len(chartjson.split(\"],\")[0].split(\"[\")) > 1:\n",
        "        jsonstripped = chartjson.split(\"],\")[0].split(\"[\")[1]\n",
        "        jsonlist = jsonstripped.split('},{')\n",
        "        print(jsonlist[0])\n",
        "        print(jsonlist[1])\n",
        "        print(jsonlist[-1])\n",
        "        print(len(jsonlist))\n",
        "        record = {}\n",
        "        for i in jsonlist:\n",
        "            print(i)\n",
        "            record['heading'] = heading\n",
        "            #split the list by comma\n",
        "            isplit = i.split(',')\n",
        "            #check there's only 4 items in the list\n",
        "            print(len(isplit))\n",
        "            #trim the text around the date\n",
        "            date = isplit[0].split('\"Date\":\"')[1].split('\"')[0]\n",
        "            #and store\n",
        "            record['Date'] = date\n",
        "            #repeat for the other 3 objects - these are numbers so don't have an opening quotation mark\n",
        "            #We convert to a float, but 0 values in CancelVeryLate cause problems here, so we can do that in the analysis \n",
        "            record['OnTime'] = float(isplit[1].split('\"OnTime\":')[1])\n",
        "            record['Late'] = float(isplit[2].split('\"Late\":')[1])\n",
        "            record['CancelVeryLate'] = isplit[3].split('\"CancelVeryLate\":')[1].replace(\"}\",\"\")\n",
        "            #store the url\n",
        "            record['url'] = fullurl\n",
        "            #grab the code for the line, which is in the last 2 slashes of the url\n",
        "            linecode = fullurl.split(\"/\")[-2]+fullurl.split(\"/\")[-1]\n",
        "            #store\n",
        "            record['code'] = linecode\n",
        "            #create a uniquecode combining that code with the date - we need this as a primary key\n",
        "            record['codeandurl'] = linecode+\"-\"+date\n",
        "            record['comments'] = ''\n",
        "            record['periodtype'] = 'day'\n",
        "            print(record)\n",
        "            print(type(record['Late']))\n",
        "            scraperwiki.sql.save(['codeandurl'],record,table_name='traindelays')\n",
        "    else:\n",
        "        print('No data')\n",
        "        record = {}\n",
        "        record['heading'] = heading\n",
        "        record['Date'] = 'NO DATA'\n",
        "        record['OnTime'] = 0.0\n",
        "        record['Late'] = 0.0\n",
        "        record['CancelVeryLate'] = 0.0\n",
        "        #store the url\n",
        "        record['url'] = fullurl\n",
        "        #grab the code for the line, which is in the last 2 slashes of the url\n",
        "        linecode = fullurl.split(\"/\")[-2]+fullurl.split(\"/\")[-1]\n",
        "        #store\n",
        "        record['code'] = linecode\n",
        "        #create a uniquecode combining that code with the date - we need this as a primary key\n",
        "        record['codeandurl'] = linecode+\"-\"+'NO DATA'\n",
        "        record['comments'] = 'EMPTY PAGE - FILTER OUT'\n",
        "        record['periodtype'] = 'day'\n",
        "        print(record)\n",
        "        scraperwiki.sql.save(['codeandurl'],record,table_name='traindelays')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohlNl5J_0A3D",
        "colab_type": "text"
      },
      "source": [
        "Then we run another loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQmM34CZ0CVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now start scraping the monthly pages\n",
        "for i in routecodes_archive:\n",
        "    fullurl = baseurl+i+\"/monthly\"\n",
        "    print(fullurl)\n",
        "    chartjson, heading = scrapechartpage(fullurl)\n",
        "    print(len(chartjson.split(\"],\")[0].split(\"[\")))\n",
        "    if len(chartjson.split(\"],\")[0].split(\"[\")) > 1:\n",
        "        jsonstripped = chartjson.split(\"],\")[0].split(\"[\")[1]\n",
        "        jsonlist = jsonstripped.split('},{')\n",
        "        print(jsonlist[0])\n",
        "        print(jsonlist[1])\n",
        "        print(jsonlist[-1])\n",
        "        print(len(jsonlist))\n",
        "        record = {}\n",
        "        for i in jsonlist:\n",
        "            print(i)\n",
        "            record['heading'] = heading\n",
        "            #split the list by comma\n",
        "            isplit = i.split(',')\n",
        "            #check there's only 4 items in the list\n",
        "            print(len(isplit))\n",
        "            #trim the text around the date\n",
        "            date = isplit[0].split('\"Date\":\"')[1].split('\"')[0]\n",
        "            #and store\n",
        "            record['Date'] = date\n",
        "            #repeat for the other 3 objects - these are numbers so don't have an opening quotation mark\n",
        "            #We convert to a float, but 0 values in CancelVeryLate cause problems here, so we can do that in the analysis \n",
        "            record['OnTime'] = float(isplit[1].split('\"OnTime\":')[1])\n",
        "            record['Late'] = float(isplit[2].split('\"Late\":')[1])\n",
        "            record['CancelVeryLate'] = isplit[3].split('\"CancelVeryLate\":')[1].replace(\"}\",\"\")\n",
        "            #store the url\n",
        "            record['url'] = fullurl\n",
        "            #grab the code for the line, which is in the last 2 slashes of the url\n",
        "            linecode = fullurl.split(\"/\")[-3]+fullurl.split(\"/\")[-2]\n",
        "            #store\n",
        "            record['code'] = linecode\n",
        "            #create a uniquecode combining that code with the date - we need this as a primary key\n",
        "            record['codeandurl'] = linecode+\"-\"+date\n",
        "            record['comments'] = ''\n",
        "            record['periodtype'] = 'month'\n",
        "            print(record)\n",
        "            print(type(record['Late']))\n",
        "            scraperwiki.sql.save(['codeandurl'],record,table_name='traindelaysmonthly')\n",
        "    else:\n",
        "        print('No data')\n",
        "        record = {}\n",
        "        record['heading'] = heading\n",
        "        record['Date'] = 'NO DATA'\n",
        "        record['OnTime'] = 0.0\n",
        "        record['Late'] = 0.0\n",
        "        record['CancelVeryLate'] = 0.0\n",
        "        #store the url\n",
        "        record['url'] = fullurl\n",
        "        #grab the code for the line, which is in the last 2 slashes of the url\n",
        "        linecode = fullurl.split(\"/\")[-3]+fullurl.split(\"/\")[-2]\n",
        "        #store\n",
        "        record['code'] = linecode\n",
        "        #create a uniquecode combining that code with the date - we need this as a primary key\n",
        "        record['codeandurl'] = linecode+\"-\"+'NO DATA'\n",
        "        record['comments'] = 'EMPTY PAGE - FILTER OUT'\n",
        "        record['periodtype'] = 'month'\n",
        "        print(record)\n",
        "        scraperwiki.sql.save(['codeandurl'],record,table_name='traindelaysmonthly')\n",
        "\n",
        "\n",
        "#jsonstripped = '{\"Date\":\"2018-09-01\",\"OnTime\":93.1,\"Late\":5.9,\"CancelVeryLate\":1},{\"Date\":\"2018-09-02\",\"OnTime\":95.5,\"Late\":4.2,\"CancelVeryLate\":0.4},{\"Date\":\"2018-09-03\",\"OnTime\":69.3,\"Late\":20.5,\"CancelVeryLate\":10.2},{\"Date\":\"2018-09-04\",\"OnTime\":87.9,\"Late\":10.9,\"CancelVeryLate\":1.2},{\"Date\":\"2018-09-05\",\"OnTime\":84.8,\"Late\":14,\"CancelVeryLate\":1.2},{\"Date\":\"2018-09-06\",\"OnTime\":86.3,\"Late\":11.3,\"CancelVeryLate\":2.4},{\"Date\":\"2018-09-07\",\"OnTime\":87.4,\"Late\":9.9,\"CancelVeryLate\":2.6},{\"Date\":\"2018-09-08\",\"OnTime\":96,\"Late\":3.6,\"CancelVeryLate\":0.4},{\"Date\":\"2018-09-09\",\"OnTime\":96.4,\"Late\":2.8,\"CancelVeryLate\":0.8},{\"Date\":\"2018-09-10\",\"OnTime\":91.5,\"Late\":7.3,\"CancelVeryLate\":1.2},{\"Date\":\"2018-09-11\",\"OnTime\":80.2,\"Late\":16.2,\"CancelVeryLate\":3.6},{\"Date\":\"2018-09-12\",\"OnTime\":88.7,\"Late\":9.9,\"CancelVeryLate\":1.4},{\"Date\":\"2018-09-13\",\"OnTime\":87.9,\"Late\":11.1,\"CancelVeryLate\":1},{\"Date\":\"2018-09-14\",\"OnTime\":80.2,\"Late\":16,\"CancelVeryLate\":3.8},{\"Date\":\"2018-09-15\",\"OnTime\":92.6,\"Late\":6.2,\"CancelVeryLate\":1.2},{\"Date\":\"2018-09-16\",\"OnTime\":91.5,\"Late\":7.7,\"CancelVeryLate\":0.8},{\"Date\":\"2018-09-17\",\"OnTime\":88.1,\"Late\":9.7,\"CancelVeryLate\":2.2},{\"Date\":\"2018-09-18\",\"OnTime\":74.1,\"Late\":21.6,\"CancelVeryLate\":4.2},{\"Date\":\"2018-09-19\",\"OnTime\":77.6,\"Late\":19.6,\"CancelVeryLate\":2.8},{\"Date\":\"2018-09-20\",\"OnTime\":81.4,\"Late\":17.4,\"CancelVeryLate\":1.2},{\"Date\":\"2018-09-21\",\"OnTime\":73.6,\"Late\":21.6,\"CancelVeryLate\":4.8},{\"Date\":\"2018-09-22\",\"OnTime\":74.4,\"Late\":14.7,\"CancelVeryLate\":11},{\"Date\":\"2018-09-23\",\"OnTime\":86.7,\"Late\":12.9,\"CancelVeryLate\":0.4},{\"Date\":\"2018-09-24\",\"OnTime\":75.5,\"Late\":20.9,\"CancelVeryLate\":3.6},{\"Date\":\"2018-09-25\",\"OnTime\":69,\"Late\":22.7,\"CancelVeryLate\":8.3},{\"Date\":\"2018-09-26\",\"OnTime\":77,\"Late\":17.8,\"CancelVeryLate\":5.3},{\"Date\":\"2018-09-27\",\"OnTime\":81.6,\"Late\":16.4,\"CancelVeryLate\":2},{\"Date\":\"2018-09-28\",\"OnTime\":87.9,\"Late\":11.3,\"CancelVeryLate\":0.8},{\"Date\":\"2018-09-29\",\"OnTime\":92.7,\"Late\":6.3,\"CancelVeryLate\":1},{\"Date\":\"2018-09-30\",\"OnTime\":93.5,\"Late\":6.5,\"CancelVeryLate\":0},{\"Date\":\"2018-10-01\",\"OnTime\":82.8,\"Late\":15.2,\"CancelVeryLate\":2},{\"Date\":\"2018-10-02\",\"OnTime\":72.3,\"Late\":24.8,\"CancelVeryLate\":2.8},{\"Date\":\"2018-10-03\",\"OnTime\":74.1,\"Late\":19.7,\"CancelVeryLate\":6.2},{\"Date\":\"2018-10-04\",\"OnTime\":79,\"Late\":15.2,\"CancelVeryLate\":5.9},{\"Date\":\"2018-10-05\",\"OnTime\":80.3,\"Late\":13.3,\"CancelVeryLate\":6.4},{\"Date\":\"2018-10-06\",\"OnTime\":83.1,\"Late\":7.7,\"CancelVeryLate\":9.2},{\"Date\":\"2018-10-07\",\"OnTime\":91.9,\"Late\":8.1,\"CancelVeryLate\":0},{\"Date\":\"2018-10-08\",\"OnTime\":83.3,\"Late\":12,\"CancelVeryLate\":4.6},{\"Date\":\"2018-10-09\",\"OnTime\":86.1,\"Late\":11.7,\"CancelVeryLate\":2.2},{\"Date\":\"2018-10-10\",\"OnTime\":84,\"Late\":14.7,\"CancelVeryLate\":1.2},{\"Date\":\"2018-10-11\",\"OnTime\":70.5,\"Late\":21.4,\"CancelVeryLate\":8.1},{\"Date\":\"2018-10-12\",\"OnTime\":43.5,\"Late\":44.7,\"CancelVeryLate\":11.9},{\"Date\":\"2018-10-13\",\"OnTime\":88.1,\"Late\":11.1,\"CancelVeryLate\":0.8},{\"Date\":\"2018-10-14\",\"OnTime\":83.2,\"Late\":13.6,\"CancelVeryLate\":3.2},{\"Date\":\"2018-10-15\",\"OnTime\":80.8,\"Late\":16.7,\"CancelVeryLate\":2.4},{\"Date\":\"2018-10-16\",\"OnTime\":82.8,\"Late\":14.9,\"CancelVeryLate\":2.2},{\"Date\":\"2018-10-17\",\"OnTime\":82.6,\"Late\":14.4,\"CancelVeryLate\":3},{\"Date\":\"2018-10-18\",\"OnTime\":87.3,\"Late\":11.7,\"CancelVeryLate\":1},{\"Date\":\"2018-10-19\",\"OnTime\":74.1,\"Late\":22,\"CancelVeryLate\":3.8},{\"Date\":\"2018-10-20\",\"OnTime\":84.6,\"Late\":14.2,\"CancelVeryLate\":1.3},{\"Date\":\"2018-10-21\",\"OnTime\":88.3,\"Late\":10.1,\"CancelVeryLate\":1.6},{\"Date\":\"2018-10-22\",\"OnTime\":69,\"Late\":21.2,\"CancelVeryLate\":9.9},{\"Date\":\"2018-10-23\",\"OnTime\":52,\"Late\":21,\"CancelVeryLate\":27},{\"Date\":\"2018-10-24\",\"OnTime\":87.7,\"Late\":10.3,\"CancelVeryLate\":2},{\"Date\":\"2018-10-25\",\"OnTime\":80.4,\"Late\":12.9,\"CancelVeryLate\":6.7},{\"Date\":\"2018-10-26\",\"OnTime\":78.6,\"Late\":19.4,\"CancelVeryLate\":2},{\"Date\":\"2018-10-27\",\"OnTime\":85.8,\"Late\":9.2,\"CancelVeryLate\":5},{\"Date\":\"2018-10-28\",\"OnTime\":88,\"Late\":7.2,\"CancelVeryLate\":4.8},{\"Date\":\"2018-10-29\",\"OnTime\":81.6,\"Late\":14.6,\"CancelVeryLate\":3.8},{\"Date\":\"2018-10-30\",\"OnTime\":85.9,\"Late\":13.3,\"CancelVeryLate\":0.8},{\"Date\":\"2018-10-30\",\"OnTime\":85.9,\"Late\":13.3,\"CancelVeryLate\":0.8},{\"Date\":\"2018-10-31\",\"OnTime\":78.8,\"Late\":16.7,\"CancelVeryLate\":4.4},{\"Date\":\"2018-10-31\",\"OnTime\":78.8,\"Late\":16.7,\"CancelVeryLate\":4.4},{\"Date\":\"2018-11-01\",\"OnTime\":78,\"Late\":15.1,\"CancelVeryLate\":6.9},{\"Date\":\"2018-11-02\",\"OnTime\":83.6,\"Late\":16.4,\"CancelVeryLate\":0},{\"Date\":\"2018-11-03\",\"OnTime\":87.5,\"Late\":8.1,\"CancelVeryLate\":4.4},{\"Date\":\"2018-11-04\",\"OnTime\":94.4,\"Late\":5.2,\"CancelVeryLate\":0.4},{\"Date\":\"2018-11-05\",\"OnTime\":85.3,\"Late\":12.3,\"CancelVeryLate\":2.4},{\"Date\":\"2018-11-06\",\"OnTime\":84.2,\"Late\":15.6,\"CancelVeryLate\":0.2},{\"Date\":\"2018-11-07\",\"OnTime\":71.7,\"Late\":25.9,\"CancelVeryLate\":2.4},{\"Date\":\"2018-11-08\",\"OnTime\":66.7,\"Late\":18.5,\"CancelVeryLate\":14.9},{\"Date\":\"2018-11-09\",\"OnTime\":63.8,\"Late\":32.1,\"CancelVeryLate\":4},{\"Date\":\"2018-11-10\",\"OnTime\":87.5,\"Late\":10,\"CancelVeryLate\":2.5},{\"Date\":\"2018-11-11\",\"OnTime\":87.9,\"Late\":12.1,\"CancelVeryLate\":0},{\"Date\":\"2018-11-12\",\"OnTime\":82,\"Late\":16.6,\"CancelVeryLate\":1.4},{\"Date\":\"2018-11-13\",\"OnTime\":83,\"Late\":14.3,\"CancelVeryLate\":2.6},{\"Date\":\"2018-11-14\",\"OnTime\":62,\"Late\":22.2,\"CancelVeryLate\":15.8},{\"Date\":\"2018-11-15\",\"OnTime\":50.3,\"Late\":35.2,\"CancelVeryLate\":14.5},{\"Date\":\"2018-11-16\",\"OnTime\":72.7,\"Late\":21.2,\"CancelVeryLate\":6.1},{\"Date\":\"2018-11-17\",\"OnTime\":74.1,\"Late\":14.8,\"CancelVeryLate\":11.1},{\"Date\":\"2018-11-18\",\"OnTime\":84.7,\"Late\":13.7,\"CancelVeryLate\":1.6},{\"Date\":\"2018-11-19\",\"OnTime\":69.5,\"Late\":28.1,\"CancelVeryLate\":2.4},{\"Date\":\"2018-11-20\",\"OnTime\":70.7,\"Late\":26.9,\"CancelVeryLate\":2.4},{\"Date\":\"2018-11-21\",\"OnTime\":78.1,\"Late\":19.5,\"CancelVeryLate\":2.4},{\"Date\":\"2018-11-22\",\"OnTime\":82.1,\"Late\":16.1,\"CancelVeryLate\":1.8},{\"Date\":\"2018-11-23\",\"OnTime\":66.5,\"Late\":31.5,\"CancelVeryLate\":2},{\"Date\":\"2018-11-24\",\"OnTime\":83.6,\"Late\":15.8,\"CancelVeryLate\":0.6},{\"Date\":\"2018-11-25\",\"OnTime\":83.4,\"Late\":14.2,\"CancelVeryLate\":2.4},{\"Date\":\"2018-11-26\",\"OnTime\":76.3,\"Late\":19.6,\"CancelVeryLate\":4},{\"Date\":\"2018-11-27\",\"OnTime\":72.3,\"Late\":25.1,\"CancelVeryLate\":2.6},{\"Date\":\"2018-11-28\",\"OnTime\":85.9,\"Late\":14.1,\"CancelVeryLate\":0}'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T689cVVFbay",
        "colab_type": "text"
      },
      "source": [
        "Now try to access that sqlite database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkt_DC1fFdR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allscrapeddata = scraperwiki.sql.select(\"* from traindelaysmonthly\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEuOMgyQFtoo",
        "colab_type": "text"
      },
      "source": [
        "Then use the `pandas` library to store the data in a data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPiHRoeIFinW",
        "colab_type": "code",
        "outputId": "ee3b7780-d139-4f69-f5f5-746eab2c0960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#import the library\n",
        "import pandas as pd\n",
        "#Create a dataframe and assign the data to it\n",
        "allscrapeddatadf = pd.DataFrame(allscrapeddata)\n",
        "#Show how many rows\n",
        "print(len(allscrapeddatadf))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmIRtCDsGYR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mQgP4SPF249",
        "colab_type": "text"
      },
      "source": [
        "We export as a CSV. Where does this go?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKkK7wuXFrTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Export as a CSV file using the .to_csv function https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html\n",
        "allscrapeddatadf.to_csv('allscrapeddata.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG8YaDZsGMt2",
        "colab_type": "text"
      },
      "source": [
        "It goes to the virtual machine. But look for a tab on the right hand side of this code, to open up a menu with 'Table of contents' - switch to 'Files' to see both the CSV file and the sqlite database.\n",
        "\n",
        "Right-click on the CSV and select Download file to get it.\n",
        "\n",
        "Alternatively you can use the [code detailed here](https://stackoverflow.com/questions/49394737/exporting-data-from-google-colab-to-local-machine), adapted below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB5TPCzJGf9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"allscrapeddata.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}